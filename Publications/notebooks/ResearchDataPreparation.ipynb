{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7247f8b-5af6-4574-9f5b-f4d41eaa9220",
   "metadata": {},
   "source": [
    "# Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4359533-1c73-4f06-b85b-89be15cc0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('./pubmed_health_articles.csv')\n",
    "Entrez.email = \"myrzakunbekovis@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a76a128-5e6e-471c-a417-d5bc28cf46c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 377010 articles for health_10-year data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:38<00:00,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 articles with 500 abstracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "HEALTH_TOPICS = {\n",
    "    \"health\": \"longevity OR aging OR healthy aging OR biohacking OR anti-aging\",\n",
    "}\n",
    "\n",
    "# Date ranges for comparison\n",
    "DATE_RANGES = {\n",
    "     \"10-year data\": (\"2015/01/01\", \"2025/07/11\"),\n",
    "#     \"pandemic\": (\"2020/01/01\", \"2022/12/31\"),\n",
    "#     \"post_pandemic\": (\"2023/01/01\", \"2025/06/21\")\n",
    "}\n",
    "\n",
    "def fetch_pubmed_data(topic_name, query, date_range, max_results=500):\n",
    "    \"\"\"Fetches PubMed articles with metadata and abstracts\"\"\"\n",
    "    # search_term = f\"({query}) AND {date_range[0]}:{date_range[1]}[PDAT]\" #we can set the conditions in search term\n",
    "\n",
    "    search_term  = f\"({query}) AND {date_range[0]}:{date_range[1]}[PDAT]\"\n",
    "    \n",
    "    # Step 1: Search PubMed\n",
    "    # handles stores the raw XML data stream\n",
    "    handle = Entrez.esearch(db=\"pubmed\",\n",
    "                            term=search_term,\n",
    "                            retmax=max_results,\n",
    "                            sort=\"relevance\",  # Get most representative articles\n",
    "                            usehistory=\"y\") #saves the search on PubMed’s history server, so you can fetch results in pages.\n",
    "    search_results = Entrez.read(handle) #reads and parses the search results\n",
    "    handle.close()\n",
    "\n",
    "    #metadata about the search\n",
    "    #needed later to retreive search results without rerunning the code\n",
    "    webenv = search_results[\"WebEnv\"] #an identifier for my search session on pubMed's server\n",
    "    query_key = search_results[\"QueryKey\"] #a reference to specific query within session\n",
    "    total = int(search_results[\"Count\"]) #total number of results matching my query\n",
    "    \n",
    "    # print(f\"Found {total} articles for {topic_name} ({date_range[0]} to {date_range[1]})\")\n",
    "\n",
    "    print(f\"Found {total} articles for {topic_name}\")\n",
    "\n",
    "    # Step 2: Batch fetching (NCBI recommends batches of 100)\n",
    "    batch_size = 100\n",
    "    articles = []\n",
    "\n",
    "    #loop runs from 0 up to the minimum between what the search found and the max number of results we wanted\n",
    "    #tqdm is needed for graphical bar progress\n",
    "    #the loop runs by fetching by 100 articles per one loop run\n",
    "    for start in tqdm(range(0, min(total, max_results), batch_size)): \n",
    "        end = min(total, start + batch_size)\n",
    "        handle = Entrez.efetch(db=\"pubmed\",\n",
    "                               retstart=start,\n",
    "                               retmax=batch_size,\n",
    "                               webenv=webenv,\n",
    "                               query_key=query_key,\n",
    "                               retmode=\"xml\")\n",
    "        data = Entrez.read(handle)\n",
    "        articles += parse_articles(data)\n",
    "        handle.close()\n",
    "        time.sleep(0.3)  # Avoid overloading server\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def parse_articles(data):\n",
    "    \"\"\"Extracts key information from PubMed XML\"\"\"\n",
    "    parsed = []\n",
    "    for article in data['PubmedArticle']:\n",
    "        try:\n",
    "            pmid = article['MedlineCitation']['PMID'].strip()\n",
    "            title = article['MedlineCitation']['Article']['ArticleTitle'].strip()\n",
    "            \n",
    "            # Abstract handling (some articles lack abstracts)\n",
    "            abstract = \"\"\n",
    "            if 'Abstract' in article['MedlineCitation']['Article']:\n",
    "                abstract_parts = article['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "                abstract = \" \".join([text for text in abstract_parts if isinstance(text, str)])\n",
    "            \n",
    "            # Journal and date info\n",
    "            journal = article['MedlineCitation']['Article']['Journal']['Title']\n",
    "            pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']\n",
    "            year = pub_date.get('Year', '')\n",
    "            \n",
    "            # MeSH terms for topic validation\n",
    "            #Medical subject headings assigned to this article\n",
    "            mesh_terms = []\n",
    "            if 'MeshHeadingList' in article['MedlineCitation']:\n",
    "                for item in article['MedlineCitation']['MeshHeadingList']:\n",
    "                    mesh_terms.append(item['DescriptorName'])\n",
    "            \n",
    "            parsed.append({\n",
    "                \"pmid\": pmid,\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"journal\": journal,\n",
    "                \"year\": year,\n",
    "                \"pub_date\": str(pub_date),\n",
    "                \"mesh_terms\": \"; \".join(mesh_terms),\n",
    "                \"source\": \"PubMed\"\n",
    "            })\n",
    "        except KeyError as e:\n",
    "            continue\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "# Collect data for all combinations\n",
    "all_data = []\n",
    "\n",
    "for topic_name, query in HEALTH_TOPICS.items():\n",
    "    for period, date_range in DATE_RANGES.items():\n",
    "        articles = fetch_pubmed_data(f\"{topic_name}_{period}\", \n",
    "                                     query, \n",
    "                                     date_range,\n",
    "                                     max_results=500)\n",
    "        for article in articles:\n",
    "            article[\"topic\"] = topic_name\n",
    "            article[\"period\"] = period\n",
    "        all_data.extend(articles)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"pubmed_health_articles_500.csv\", index=False)\n",
    "print(f\"Saved {len(df)} articles with {len(df) - df['abstract'].isna().sum()} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06f7fe-53f0-448f-aa55-19f47363705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'PubmedArticle': [\n",
    "        {\n",
    "            'MedlineCitation': {\n",
    "                'PMID': '12345678',\n",
    "                'Article': {\n",
    "                    'ArticleTitle': 'Example title',\n",
    "                    'Abstract': {\n",
    "                        'AbstractText': ['This is the abstract text.']\n",
    "                    },\n",
    "                    'Journal': {\n",
    "                        'Title': 'Example Journal',\n",
    "                        'JournalIssue': {\n",
    "                            'PubDate': {\n",
    "                                'Year': '2022',\n",
    "                                'Month': 'Nov',\n",
    "                                'Day': '15'\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'MeshHeadingList': [\n",
    "                    {'DescriptorName': 'Vaccination'},\n",
    "                    {'DescriptorName': 'Public Health'}\n",
    "                ]\n",
    "            },\n",
    "            'PubmedData': {\n",
    "                \n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68180445-d6c9-4eab-9e8a-c9d29bcfb8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['topic', 'period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f508d2d-ee4f-48ea-b145-3d38494298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./pubmed_health_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8795a-e20a-41c4-b4c3-265828fb94a3",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf5ee6a-0834-443a-b07f-4ec25158a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NYT months:   0%|                                       | 0/187 [00:00<?, ?it/s]/tmp/ipykernel_21321/3836959141.py:46: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  return df[text.str.contains(KEYWORDS, regex=True)]\n",
      "NYT months:  99%|██████████████████████████▊| 186/187 [1:32:44<00:27, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error 2025-06: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/6.json?api-key=9ivauV5adGK29eJ5Tq9q0ImuQ75HGnt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NYT months: 100%|███████████████████████████| 187/187 [1:32:57<00:00, 22.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error 2025-07: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/7.json?api-key=9ivauV5adGK29eJ5Tq9q0ImuQ75HGnt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NYT months: 100%|███████████████████████████| 187/187 [1:33:10<00:00, 29.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  Saved 386 filtered NYT articles to nyt_longevity_2010‑present.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "nyt_longevity_scraper.py\n",
    "------------------------\n",
    "NYT Archive API pull (2010‑present) filtered for longevity / healthy‑aging\n",
    "keywords.   Requires: python-dotenv, requests, pandas, tqdm.\n",
    "\"\"\"\n",
    "import os, re, time, datetime as dt, requests, pandas as pd\n",
    "from dotenv import load_dotenv                         # pip install python-dotenv\n",
    "from tqdm import tqdm                                  # pip install tqdm\n",
    "\n",
    "# ── 0.  CONFIG ────────────────────────────────────────────────────────────────\n",
    "API_KEY = \"9ivauV5adGK29eJ5Tq9q0ImuQ75HGnt2\"                 # put your key in .env file\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set NYT_API_KEY in your .env file\")\n",
    "\n",
    "BASE  = \"https://api.nytimes.com/svc\"\n",
    "OUT_DIR = \"nyt_monthly_csv\"                            # where monthly files live\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# keyword regex (add more if desired)\n",
    "KEYWORDS = re.compile(\n",
    "    r\"\\b(longevity|healthy[ -]ag(?:ing|e?ing)|biohacking|anti[ -]ag(?:ing|e?ing))\\b\",\n",
    "    flags=re.I\n",
    ")\n",
    "\n",
    "session = requests.Session()\n",
    "session.params = {\"api-key\": API_KEY}                  # append key automatically\n",
    "\n",
    "\n",
    "# ── 1.  HELPERS ───────────────────────────────────────────────────────────────\n",
    "def archive_month(year: int, month: int) -> pd.DataFrame:\n",
    "    \"\"\"Fetch one month; return *raw* DataFrame.\"\"\"\n",
    "    url = f\"{BASE}/archive/v1/{year}/{month}.json\"\n",
    "    r = session.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    docs = r.json()[\"response\"][\"docs\"]\n",
    "    return pd.json_normalize(docs)\n",
    "\n",
    "def filter_keywords(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Keep rows whose text fields match KEYWORDS regex.\"\"\"\n",
    "    text = (\n",
    "        df[\"abstract\"].fillna(\"\") + \" \" +\n",
    "        df[\"lead_paragraph\"].fillna(\"\") + \" \" +\n",
    "        df[\"snippet\"].fillna(\"\")\n",
    "    )\n",
    "    return df[text.str.contains(KEYWORDS, regex=True)]\n",
    "\n",
    "def month_file(year: int, month: int) -> str:\n",
    "    \"\"\"Path for monthly CSV.\"\"\"\n",
    "    return f\"{OUT_DIR}/nyt_{year}_{month:02d}.csv\"\n",
    "\n",
    "\n",
    "# ── 2.  MAIN LOOP ─────────────────────────────────────────────────────────────\n",
    "def collect_nyt_aging(start_year: int = 2010):\n",
    "    today = dt.datetime.today()\n",
    "    months_total = (today.year - start_year) * 12 + today.month\n",
    "    progress = tqdm(total=months_total, desc=\"NYT months\")\n",
    "\n",
    "    for year in range(start_year, today.year + 1):\n",
    "        max_month = today.month if year == today.year else 12\n",
    "        for month in range(1, max_month + 1):\n",
    "            progress.update(1)\n",
    "            fn = month_file(year, month)\n",
    "            if os.path.exists(fn):                     # resume support\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df_raw  = archive_month(year, month)\n",
    "                df_keep = filter_keywords(df_raw)\n",
    "\n",
    "                if not df_keep.empty:\n",
    "                    df_keep.to_csv(fn, index=False)\n",
    "            except requests.HTTPError as e:\n",
    "                print(f\"HTTP error {year}-{month:02d}: {e}\")\n",
    "            except Exception as ex:\n",
    "                print(f\"Other error {year}-{month:02d}: {ex}\")\n",
    "\n",
    "            time.sleep(12.5)  # 5 calls/min ⇒ 12 s per call keeps us safe\n",
    "\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "# ── 3.  CONCAT AFTER RUN ─────────────────────────────────────────────────────\n",
    "def concat_all() -> pd.DataFrame:\n",
    "    frames = [\n",
    "        pd.read_csv(os.path.join(OUT_DIR, f))\n",
    "        for f in os.listdir(OUT_DIR)\n",
    "        if f.endswith(\".csv\")\n",
    "    ]\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "\n",
    "# ── 4.  RUN ───────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    collect_nyt_aging(start_year=2010)\n",
    "    df_all = concat_all()\n",
    "    df_all.to_csv(\"nyt_longevity_2010‑present.csv\", index=False)\n",
    "    print(f\"✔  Saved {len(df_all):,} filtered NYT articles to nyt_longevity_2010‑present.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f936e0-32c3-4cb1-9bce-37da5043d270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic-env)",
   "language": "python",
   "name": "bertopic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
